{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5d2303",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eca2b0",
   "metadata": {},
   "source": [
    "Classification is a supervised learning approach.\n",
    "Its aim is to categorize unknown items into a discrete set of categories or 'classes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331c113",
   "metadata": {},
   "source": [
    "The target attribute is the categorical variable with either discrete values or a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5cd69",
   "metadata": {},
   "source": [
    "Many problems can be expressed as associations between features and target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314898a2",
   "metadata": {},
   "source": [
    "### Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd844ee",
   "metadata": {},
   "source": [
    "* Decision Trees\n",
    "* Naive Bayes\n",
    "* Linear Discrete Analysis\n",
    "* K-Nearest Neighbour\n",
    "* Logistic Regression\n",
    "* Neural Networks\n",
    "* Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b79e5f",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a5121",
   "metadata": {},
   "source": [
    "KNN can be summarized as predicting the target attribute of an observation by assessing the target variables of its x nearest neighbours given a set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f66d56",
   "metadata": {},
   "source": [
    "on a KNN plot similar cases aka. those with similar features near each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5082cd",
   "metadata": {},
   "source": [
    "The distance between two points is calculated using euclidian distance aka. square of x + square of y = square of z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4fcee6",
   "metadata": {},
   "source": [
    "The algorithm is then:\n",
    "\n",
    "1) Pick a val for K\n",
    "\n",
    "2) Calculate the distance of unknown cases from all cases\n",
    "\n",
    "3) Select k observations in the training data that are nearest to the unknown data point.\n",
    "\n",
    "4) Predict the response of the unknown data point using most popular response values from K-nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d148b79",
   "metadata": {},
   "source": [
    "For choosing the value of K, the model can be evaluated for accuracy using a train test split and increasing K until accuracy decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0123f6b",
   "metadata": {},
   "source": [
    "### Evaluation metrics in KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d787dea",
   "metadata": {},
   "source": [
    "evaluation of accuracy is the value of the actual labels vs. the predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e9d38",
   "metadata": {},
   "source": [
    "There are several versions of this metric. The simplest is Jaccard Index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef88b05",
   "metadata": {},
   "source": [
    "#### Jaccard Index\n",
    "\n",
    "The Jaccard Index is the size of the intersection / the size of the union of the actual and predicted values\n",
    "\n",
    "EG. in a dataset of 10 with 8 correct predictions the accuracy is\n",
    "\n",
    "8/ 10 + 10 - 8 = .66"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548524b4",
   "metadata": {},
   "source": [
    "#### F1-Score | Confusion Matrix\n",
    "\n",
    "The f1 score on a confusion matrix plots the outcomes and possible outcomes for a binary set on 2 axis.\n",
    "\n",
    "for example\n",
    "\n",
    "6 9\n",
    "4 7\n",
    "\n",
    "in this matrix the first row shows the outcomes of predicting 1.\n",
    "Their are 15 1 vals in total (6+9) the model predicts 6 and incorrectly predicts 9 as zero.\n",
    "\n",
    "in the second row there are 11 total 0 values, the model correctly predicts 7 as 0 and incorrectly predicts 4 as 1.\n",
    "\n",
    "These are the counts of true positives, false positives, true negatives and false negatives.\n",
    "\n",
    "THe precision of the model is scored as:\n",
    "\n",
    "TP / (TP * FP)\n",
    "\n",
    "and recall is defined as\n",
    "\n",
    "TP / (TP + FN)\n",
    "\n",
    "which is calculated for each class\n",
    "\n",
    "The F1 score is then\n",
    "\n",
    "2x (prc * rec) / (prc + rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9678771",
   "metadata": {},
   "source": [
    "#### Log Loss\n",
    "\n",
    "Sometimes the output of a classifier is the probability of a class label instead of the label itself.\n",
    "\n",
    "The out of logistic regression the output can be the probablity.\n",
    "\n",
    "Logarithmic Loss measures accuracy where the predicted outcome is a probability between 0 and 1\n",
    "\n",
    "the log loss equation can be applied to each prediction using the log loss equation\n",
    "\n",
    "(y * log(y_hat) + (1 - y) * log(1 - y_hat))\n",
    "\n",
    "Then the average logloss is calculated. the smaller the log loss, the higher the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041f8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
